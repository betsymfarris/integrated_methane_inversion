#!/bin/bash

#SBATCH --ntasks=8 #you get 3.75 GRAM per ntask
#SBATCH --nodes=1
#SBATCH --time=1:00:00
#SBATCH --partition=atesting
#SBATCH --qos=testing
#SBATCH --output=geoschem_error_%j.log
#SBATCH --job-name=geoschem
#SBATCH --mail-type=END

###############################################################################
### Sample GEOS-Chem Classic run script for Harvard Cannon (using SLURM).
###
### If you are running a nested-grid simulation at fine resolution, you
### will likely need to request additional memory, cores, and time.
###
### -c           : Requests this many cores
### -N           : Requests a single node
### --mem        : Requests this amount of memory in GB
### -p           : Requests these partitions where the job can run
### -t           : Requests time for the job (days-hours:minutes)
###  --exclusive : Reserves entire nodes (i.e. to prevent backfilling jobs)
###############################################################################

module purge
module load anaconda/2023.09 spack/0.20.1
# Read in the config file and source the environment file
condaEnv=$(grep -Po 'condaEnv:\s*\K.*' /projects/befa4441/integrated_methane_inversion/src/write_BCs/config_boundary_conditions_bf.yml)
condaFile=$(grep -Po 'condaFile:\s*\K.*' /projects/befa4441/integrated_methane_inversion/src/write_BCs/config_boundary_conditions_bf.yml)
condaFile=$(eval echo "$condaFile")
source ${condaFile}
conda activate ${condaEnv}
eval $(python /projects/befa4441/integrated_methane_inversion/src/utilities/parse_yaml.py /projects/befa4441/integrated_methane_inversion/src/write_BCs/config_boundary_conditions_bf.yml)
source ${geosChemEnv}

# Information needed for GEOS-Chem simulations
export GC_USER_REGISTERED=true
export GC_DATA_ROOT="${geosChemDataPath}"

# Set the proper # of threads for OpenMP
# SLURM_CPUS_PER_TASK ensures this matches the number you set with -c above
#export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OMP_NUM_THREADS=1

# Run GEOS_Chem.  The "time" command will return CPU and wall times.
# Stdout and stderr will be directed to the "GC.log" log file
# (you can change the log file name below if you wish)
# srun -c $OMP_NUM_THREADS time -p ./gcclassic >> GC.log

export SLURM_EXPORT_ENV=ALL
env
mpirun -n $SLURM_NTASKS ./gcclassic 

# Exit normally
exit 0

