#!/bin/bash

#SBATCH --cpus-per-task=8
#SBATCH -N 1
#SBATCH -t 0-12:00
#SBATCH -p amilan128c
#SBATCH --qos=normal
#SBATCH --mem=15000
#SBATCH -o geoschem_error.log
#SBATCH --mail-type=END

###############################################################################
### Sample GEOS-Chem Classic run script for Harvard Cannon (using SLURM).
###
### If you are running a nested-grid simulation at fine resolution, you
### will likely need to request additional memory, cores, and time.
###
### -c           : Requests this many cores
### -N           : Requests a single node
### --mem        : Requests this amount of memory in GB
### -p           : Requests these partitions where the job can run
### -t           : Requests time for the job (days-hours:minutes)
###  --exclusive : Reserves entire nodes (i.e. to prevent backfilling jobs)
###############################################################################

# Set the proper # of threads for OpenMP
# SLURM_CPUS_PER_TASK ensures this matches the number you set with -c above
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

# Run GEOS_Chem.  The "time" command will return CPU and wall times.
# Stdout and stderr will be directed to the "GC.log" log file
# (you can change the log file name below if you wish)
# srun -c $OMP_NUM_THREADS time -p ./gcclassic >> GC.log
srun /bin/time -p ./gcclassic >> GC.log

# Exit normally
exit 0

